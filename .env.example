# Environment variables for vLLM server configuration
# Copy this to .env and fill in your values:
#   cp .env.example .env
#   nano .env  # Add your HF_TOKEN

# ============================================
# REQUIRED
# ============================================

# Hugging Face token (required for gated/private models like gpt-oss-20)
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=

# ============================================
# MODEL CONFIGURATION
# ============================================

# Model to load from Hugging Face
MODEL=gpt-oss-20

# Server host and port
HOST=0.0.0.0
PORT=8000

# ============================================
# PERFORMANCE TUNING
# ============================================

# GPU memory utilization (0.0 to 1.0)
# Higher = more KV cache = more concurrent requests
# Start with 0.90, increase to 0.95 if stable
GPU_MEM_UTIL=0.90

# Maximum context length (prompt + generation)
# Smaller = more concurrent requests fit in memory
# Common values: 2048, 4096, 8192
MAX_MODEL_LEN=4096

# KV cache data type
# Options: auto, fp16, fp8 (fp8 requires H100/A100, halves memory usage)
KV_CACHE_DTYPE=auto

# ============================================
# BATCHING (OPTIONAL)
# ============================================

# Maximum number of sequences processed concurrently
# MAX_NUM_SEQS=256

# Maximum tokens in a single batch (prefill + decode)
# Higher = better throughput, higher latency variance
# MAX_NUM_BATCHED_TOKENS=8192

# ============================================
# BENCHMARKING (OPTIONAL)
# ============================================

# Enable GPU telemetry monitoring during benchmarks
# Requires: pip install nvidia-ml-py3
# ENABLE_TELEMETRY=false
