# vLLM Benchmark Scenarios Configuration
# 
# This file defines benchmark scenarios for testing vLLM inference performance
# across different workload patterns (short chat, RAG, long context, etc.)

# Global defaults applied to all scenarios
defaults:
  base_url: "http://localhost:8000/v1"
  model: "facebook/opt-1.3b"
  temperature: 0.0
  stream: true

# Benchmark scenarios
# Each scenario tests a different workload pattern with varying:
# - prefill_tokens: approximate number of input tokens (prompt length)
# - max_new_tokens: maximum number of tokens to generate
# - concurrencies: list of concurrent request levels to test
# - requests_per_user: requests per concurrent user (total scales with concurrency)
# - num_requests: DEPRECATED - use requests_per_user for scaled workload
#
# RAG-specific fields:
# - rag: true/false - whether to use pre-generated RAG prompts from JSONL
# - prompt_file: path to JSONL file with pre-generated prompts
# - target_prefill_tokens: target token count for Wikipedia context + question + history
# - history_turns: number of conversation turns to include before new query
# - dolly_questions: whether Dolly questions are used (informational only)

scenarios:
  # Realistic RAG with Wikipedia context + Dolly questions + conversation history
  # Uses pre-generated JSONL prompts with actual tokenized lengths
  # Total requests scale with concurrency (e.g., 4 users × 10 req/user = 40 requests)
  rag_realistic:
    description: "Realistic RAG: Wikipedia context + Dolly questions + conversation history"
    rag: true
    prompt_file: "../data/rag_prompts_500.jsonl"
    prefill_tokens: 1700  # Target prefill (required for validation)
    target_prefill_tokens: 1700  # 1.5k-1.9k range
    history_turns: 2
    dolly_questions: true
    max_new_tokens: 256
    concurrencies: [4, 8, 16, 32]
    num_requests: 10  # Minimum requests (will scale with concurrency if requests_per_user is set)
    requests_per_user: 10  # Each user sends 10 requests
    # Total requests: 4×10=40, 8×10=80, 16×10=160, 32×10=320

  # Heavy RAG with more context and history
  # Heavy RAG workload for stress testing
  rag_heavy:
    description: "Heavy RAG: Large context + high concurrency"
    rag: true
    prompt_file: "../data/rag_prompts_500.jsonl"
    prefill_tokens: 3000  # Target prefill (required for validation)
    target_prefill_tokens: 3000
    history_turns: 4
    dolly_questions: true
    max_new_tokens: 512
    concurrencies: [8, 16, 32]
    num_requests: 20  # Minimum requests (will scale with concurrency)
    requests_per_user: 20
    # Total requests: 4×8=32, 8×8=64, 16×8=128

  # Legacy short_chat (kept for backward compatibility, uses fixed workload)
  short_chat:
    description: "Short chat messages with brief responses (legacy fixed workload)"
    rag: false
    prefill_tokens: 256
    max_new_tokens: 128
    concurrencies: [8, 16, 32, 64]
    num_requests: 500  # Fixed total, not scaled
